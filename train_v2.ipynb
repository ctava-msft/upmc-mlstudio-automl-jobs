{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bd82bd",
   "metadata": {},
   "source": [
    "# Azure Machine Learning AutoML Training Notebook (SDK v2)\n",
    "\n",
    "This notebook provides an interactive version of the AutoML training pipeline with better storage integration and simplified APIs using Azure ML SDK v2.\n",
    "\n",
    "## Overview\n",
    "- Load configuration from YAML\n",
    "- Connect to Azure ML workspace\n",
    "- Load and preprocess data\n",
    "- Create or retrieve data assets\n",
    "- Configure AutoML job\n",
    "- Submit and monitor training\n",
    "- Generate model explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892e170",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml import automl\n",
    "from azure.ai.ml.entities import Environment, Data\n",
    "from azure.identity import DefaultAzureCredential, AzureCliCredential, InteractiveBrowserCredential\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from feature_engineering import FeatureEngineer, validate_data\n",
    "\n",
    "# Interpretability imports\n",
    "try:\n",
    "    from interpret.ext.blackbox import TabularExplainer\n",
    "    import shap\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    INTERPRETABILITY_AVAILABLE = True\n",
    "    print(\"✓ Interpretability libraries loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠ Interpretability libraries not available: {e}\")\n",
    "    INTERPRETABILITY_AVAILABLE = False\n",
    "\n",
    "print(\"✓ All required libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7a076",
   "metadata": {},
   "source": [
    "## 2. Setup Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada29bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(config: dict) -> logging.Logger:\n",
    "    \"\"\"Setup logging configuration\"\"\"\n",
    "    log_level = config.get('output', {}).get('log_level', 'INFO')\n",
    "    log_to_file = config.get('output', {}).get('log_to_file', False)\n",
    "    log_file_path = config.get('output', {}).get('log_file_path', './logs/training.log')\n",
    "    \n",
    "    # Create logs directory if needed\n",
    "    if log_to_file:\n",
    "        Path(log_file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Configure logging\n",
    "    handlers = [logging.StreamHandler()]\n",
    "    if log_to_file:\n",
    "        handlers.append(logging.FileHandler(log_file_path))\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, log_level),\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=handlers,\n",
    "        force=True  # Override existing configuration\n",
    "    )\n",
    "    \n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Logging setup function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccdc22",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66119d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Load configuration from YAML\n",
    "config_path = \"config.yaml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging(config)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Azure ML AutoML Training (SDK v2)\")\n",
    "print(f\"Experiment: {config['experiment']['name']}\")\n",
    "print(f\"Task Type: {config['automl']['task']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288ce80",
   "metadata": {},
   "source": [
    "## 4. Azure Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c8191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credential(auth_method: str):\n",
    "    \"\"\"Get appropriate Azure credential based on configuration\"\"\"\n",
    "    print(f\"Authentication method: {auth_method}\")\n",
    "    \n",
    "    if auth_method == \"cli\":\n",
    "        print(\"Using Azure CLI authentication\")\n",
    "        return AzureCliCredential()\n",
    "    elif auth_method == \"interactive\":\n",
    "        print(\"Using Interactive Browser authentication\")\n",
    "        return InteractiveBrowserCredential()\n",
    "    else:  # default\n",
    "        print(\"Using Default Azure Credential (tries multiple methods)\")\n",
    "        return DefaultAzureCredential()\n",
    "\n",
    "# Get credentials\n",
    "auth_config = config.get('authentication', {})\n",
    "auth_method = auth_config.get('method', 'default').lower()\n",
    "credential = get_credential(auth_method)\n",
    "\n",
    "print(\"✓ Credentials obtained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2cdba1",
   "metadata": {},
   "source": [
    "## 5. Connect to Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get workspace details from environment\n",
    "subscription_id = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "workspace_name = os.getenv(\"AZURE_ML_WORKSPACE\")\n",
    "\n",
    "if not all([subscription_id, resource_group, workspace_name]):\n",
    "    raise ValueError(\"Missing Azure credentials in .env file. Please check AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, and AZURE_ML_WORKSPACE\")\n",
    "\n",
    "print(\"Connecting to Azure ML Workspace...\")\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group_name=resource_group,\n",
    "    workspace_name=workspace_name\n",
    ")\n",
    "\n",
    "print(f\"✓ Connected to workspace: {workspace_name}\")\n",
    "print(f\"  Resource Group: {resource_group}\")\n",
    "print(f\"  Subscription: {subscription_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1eac8",
   "metadata": {},
   "source": [
    "## 6. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5876925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(config: dict) -> pd.DataFrame:\n",
    "    \"\"\"Load and preprocess data according to configuration\"\"\"\n",
    "    data_config = config['data']\n",
    "    input_path = data_config['input_path']\n",
    "    \n",
    "    print(f\"Loading data from {input_path}\")\n",
    "    \n",
    "    # Determine file type and load appropriately\n",
    "    if input_path.endswith('.csv'):\n",
    "        df = pd.read_csv(input_path)\n",
    "    elif input_path.endswith('.parquet'):\n",
    "        df = pd.read_parquet(input_path)\n",
    "    elif input_path.endswith('.xlsx'):\n",
    "        df = pd.read_excel(input_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {input_path}\")\n",
    "    \n",
    "    print(f\"✓ Data loaded. Shape: {df.shape}\")\n",
    "    \n",
    "    # Validate data\n",
    "    validation_config = data_config.get('validation', {})\n",
    "    if validation_config:\n",
    "        print(\"Validating data...\")\n",
    "        df = validate_data(df, validation_config)\n",
    "        print(f\"✓ Data validated. Shape after validation: {df.shape}\")\n",
    "    \n",
    "    # Drop specified columns\n",
    "    columns_to_drop = data_config.get('columns_to_drop', [])\n",
    "    if columns_to_drop:\n",
    "        existing_cols = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_cols:\n",
    "            df = df.drop(columns=existing_cols)\n",
    "            print(f\"✓ Dropped {len(existing_cols)} columns\")\n",
    "    \n",
    "    # Apply feature engineering\n",
    "    fe_config = config.get('feature_engineering', {})\n",
    "    if fe_config.get('enabled', True):\n",
    "        print(\"Applying feature engineering...\")\n",
    "        engineer = FeatureEngineer(fe_config)\n",
    "        df = engineer.apply_transformations(df)\n",
    "        print(f\"✓ Feature engineering complete. Final shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Check if we should use existing dataset or load from file\n",
    "data_config = config['data']\n",
    "use_existing = data_config.get('use_existing_dataset', False)\n",
    "\n",
    "if use_existing:\n",
    "    print(f\"Will use existing dataset: {data_config['dataset_name']}\")\n",
    "    df = None\n",
    "else:\n",
    "    df = load_and_preprocess_data(config)\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13cd2d7",
   "metadata": {},
   "source": [
    "## 7. Save Preprocessed Data (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    output_config = config.get('output', {})\n",
    "    if output_config.get('save_preprocessed_data', True):\n",
    "        preprocessed_dir = Path(output_config.get('preprocessed_data_path', './data/preprocessed'))\n",
    "        preprocessed_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        preprocessed_path = preprocessed_dir / f\"preprocessed_data_{timestamp}.csv\"\n",
    "        df.to_csv(preprocessed_path, index=False)\n",
    "        print(f\"✓ Preprocessed data saved to: {preprocessed_path}\")\n",
    "    else:\n",
    "        # Create temporary file\n",
    "        preprocessed_path = f\"temp_preprocessed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        df.to_csv(preprocessed_path, index=False)\n",
    "        print(f\"✓ Temporary file created: {preprocessed_path}\")\n",
    "else:\n",
    "    preprocessed_path = None\n",
    "    print(\"Using existing dataset - no preprocessing needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc4f9f0",
   "metadata": {},
   "source": [
    "## 8. Get or Create Data Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_data_asset(ml_client: MLClient, data_path: str, config: dict) -> Data:\n",
    "    \"\"\"Get existing data asset or create new one\"\"\"\n",
    "    data_config = config['data']\n",
    "    dataset_name = data_config['dataset_name']\n",
    "    use_existing = data_config.get('use_existing_dataset', False)\n",
    "    \n",
    "    if use_existing:\n",
    "        # Get existing data asset by name\n",
    "        dataset_version = data_config.get('dataset_version', 'latest')\n",
    "        \n",
    "        if dataset_version == 'latest':\n",
    "            version_str = None\n",
    "        else:\n",
    "            version_str = str(dataset_version)\n",
    "        \n",
    "        print(f\"Getting existing data asset: {dataset_name} (version: {dataset_version})\")\n",
    "        \n",
    "        try:\n",
    "            if version_str:\n",
    "                data_asset = ml_client.data.get(name=dataset_name, version=version_str)\n",
    "            else:\n",
    "                data_asset = ml_client.data.get(name=dataset_name, label=\"latest\")\n",
    "            \n",
    "            print(f\"✓ Data asset obtained: {dataset_name} v{data_asset.version}\")\n",
    "            return data_asset\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to get data asset '{dataset_name}': {str(e)}\")\n",
    "            print(\"Tip: Verify the data asset exists in Azure ML Studio\")\n",
    "            raise\n",
    "    \n",
    "    # Create new data asset from local file\n",
    "    print(f\"Creating new data asset from: {data_path}\")\n",
    "    \n",
    "    data_asset = Data(\n",
    "        name=dataset_name,\n",
    "        description=data_config.get('dataset_description', 'Training dataset'),\n",
    "        path=data_path,\n",
    "        type=AssetTypes.URI_FILE\n",
    "    )\n",
    "    \n",
    "    # SDK v2 automatically uploads and registers the data asset\n",
    "    print(\"Uploading and registering data asset...\")\n",
    "    data_asset = ml_client.data.create_or_update(data_asset)\n",
    "    \n",
    "    print(f\"✓ Data asset created: {dataset_name} v{data_asset.version}\")\n",
    "    return data_asset\n",
    "\n",
    "# Get or create the data asset\n",
    "data_asset = get_or_create_data_asset(ml_client, str(preprocessed_path) if preprocessed_path else None, config)\n",
    "print(f\"\\nData Asset Details:\")\n",
    "print(f\"  Name: {data_asset.name}\")\n",
    "print(f\"  Version: {data_asset.version}\")\n",
    "print(f\"  Type: {data_asset.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9135d11",
   "metadata": {},
   "source": [
    "## 9. Get or Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_environment(ml_client: MLClient, env_name: str, conda_file_path: str) -> Environment:\n",
    "    \"\"\"Get existing environment or create new one from conda file\"\"\"\n",
    "    env_version = \"1\"\n",
    "    \n",
    "    # Try to get existing environment\n",
    "    try:\n",
    "        print(f\"Checking for existing environment: {env_name}:{env_version}\")\n",
    "        env = ml_client.environments.get(name=env_name, version=env_version)\n",
    "        print(f\"✓ Environment found: {env_name}:{env_version}\")\n",
    "        return env\n",
    "    except Exception as e:\n",
    "        print(f\"Environment not found, creating new one: {str(e)}\")\n",
    "    \n",
    "    # Create new environment from conda file\n",
    "    print(f\"Creating new environment from: {conda_file_path}\")\n",
    "    \n",
    "    # Verify conda file exists\n",
    "    if not os.path.exists(conda_file_path):\n",
    "        raise FileNotFoundError(f\"Conda file not found: {conda_file_path}\")\n",
    "    \n",
    "    # Create environment with curated base image\n",
    "    env = Environment(\n",
    "        name=env_name,\n",
    "        description=\"AutoML training environment with SDK v2\",\n",
    "        conda_file=conda_file_path,\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    "        version=env_version\n",
    "    )\n",
    "    \n",
    "    print(\"Registering environment with Azure ML workspace...\")\n",
    "    env = ml_client.environments.create_or_update(env)\n",
    "    \n",
    "    print(f\"✓ Environment created: {env_name}:{env.version}\")\n",
    "    return env\n",
    "\n",
    "# Get or create environment\n",
    "env_name = config.get('environment', {}).get('name', 'secondary-cvd-risk')\n",
    "conda_file = config.get('environment', {}).get('conda_file', 'conda_env_v_1_0_0.yml')\n",
    "environment = get_or_create_environment(ml_client, env_name, conda_file)\n",
    "\n",
    "print(f\"\\nEnvironment Details:\")\n",
    "print(f\"  Name: {environment.name}\")\n",
    "print(f\"  Version: {environment.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64add999",
   "metadata": {},
   "source": [
    "## 10. Create AutoML Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cefb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_automl_job(ml_client: MLClient, data_asset: Data, config: dict, environment: Environment = None):\n",
    "    \"\"\"Create AutoML classification/regression/forecasting job using SDK v2\"\"\"\n",
    "    automl_cfg = config['automl']\n",
    "    data_config = config['data']\n",
    "    compute_config = config['compute']\n",
    "    experiment_config = config['experiment']\n",
    "    \n",
    "    task_type = automl_cfg['task'].lower()\n",
    "    \n",
    "    print(f\"Creating AutoML {task_type} job\")\n",
    "    if environment:\n",
    "        print(f\"Using environment: {environment.name}:{environment.version}\")\n",
    "    \n",
    "    # Prepare training data input\n",
    "    training_data_input = Input(\n",
    "        type=AssetTypes.MLTABLE if data_asset.type == AssetTypes.MLTABLE else AssetTypes.URI_FILE,\n",
    "        path=f\"azureml:{data_asset.name}:{data_asset.version}\"\n",
    "    )\n",
    "    \n",
    "    # Create base job configuration based on task type\n",
    "    if task_type == \"classification\":\n",
    "        job = automl.classification(\n",
    "            compute=compute_config['cluster_name'],\n",
    "            experiment_name=experiment_config['name'],\n",
    "            training_data=training_data_input,\n",
    "            target_column_name=data_config['label_column'],\n",
    "            primary_metric=automl_cfg['primary_metric'],\n",
    "            n_cross_validations=automl_cfg['training'].get('n_cross_validations', 5),\n",
    "            enable_model_explainability=True,\n",
    "            tags={\"framework\": \"AutoML\", \"sdk_version\": \"v2\"}\n",
    "        )\n",
    "    elif task_type == \"regression\":\n",
    "        job = automl.regression(\n",
    "            compute=compute_config['cluster_name'],\n",
    "            experiment_name=experiment_config['name'],\n",
    "            training_data=training_data_input,\n",
    "            target_column_name=data_config['label_column'],\n",
    "            primary_metric=automl_cfg['primary_metric'],\n",
    "            n_cross_validations=automl_cfg['training'].get('n_cross_validations', 5),\n",
    "            enable_model_explainability=True,\n",
    "            tags={\"framework\": \"AutoML\", \"sdk_version\": \"v2\"}\n",
    "        )\n",
    "    elif task_type == \"forecasting\":\n",
    "        job = automl.forecasting(\n",
    "            compute=compute_config['cluster_name'],\n",
    "            experiment_name=experiment_config['name'],\n",
    "            training_data=training_data_input,\n",
    "            target_column_name=data_config['label_column'],\n",
    "            primary_metric=automl_cfg['primary_metric'],\n",
    "            n_cross_validations=automl_cfg['training'].get('n_cross_validations', 5),\n",
    "            tags={\"framework\": \"AutoML\", \"sdk_version\": \"v2\"}\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "    \n",
    "    # Set training limits\n",
    "    training_cfg = automl_cfg['training']\n",
    "    job.set_limits(\n",
    "        timeout_minutes=training_cfg.get('experiment_timeout_minutes', 60),\n",
    "        trial_timeout_minutes=training_cfg.get('iteration_timeout_minutes', 20),\n",
    "        max_trials=training_cfg.get('max_concurrent_iterations', 4),\n",
    "        enable_early_termination=training_cfg.get('enable_early_stopping', True)\n",
    "    )\n",
    "    \n",
    "    # Configure featurization\n",
    "    featurization_cfg = automl_cfg.get('featurization', {})\n",
    "    if featurization_cfg.get('mode') == 'off':\n",
    "        job.set_featurization(enable_dnn_featurization=False)\n",
    "    \n",
    "    # Set allowed/blocked models\n",
    "    models_cfg = automl_cfg.get('models', {})\n",
    "    if models_cfg.get('allowed'):\n",
    "        job.set_training(allowed_training_algorithms=models_cfg['allowed'])\n",
    "    if models_cfg.get('blocked'):\n",
    "        job.set_training(blocked_training_algorithms=models_cfg['blocked'])\n",
    "    \n",
    "    print(f\"✓ AutoML job configured: {task_type.capitalize()}, Primary Metric: {automl_cfg['primary_metric']}\")\n",
    "    return job\n",
    "\n",
    "# Create the AutoML job\n",
    "job = create_automl_job(ml_client, data_asset, config, environment)\n",
    "print(\"\\n✓ AutoML job created and ready to submit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13962628",
   "metadata": {},
   "source": [
    "## 11. Submit AutoML Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting AutoML job...\")\n",
    "\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"✓ Job submitted successfully!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Job name: {returned_job.name}\")\n",
    "print(f\"  Job ID: {returned_job.id}\")\n",
    "print(f\"  Status: {returned_job.status}\")\n",
    "print(f\"  Studio URL: {returned_job.studio_url}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd9eed",
   "metadata": {},
   "source": [
    "## 12. Monitor Job Progress (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a343ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to monitor the job and wait for completion\n",
    "# This will block until the job completes\n",
    "\n",
    "print(\"Monitoring job progress...\")\n",
    "print(\"You can monitor the job in Azure ML Studio:\")\n",
    "print(f\"  {returned_job.studio_url}\")\n",
    "print(\"\")\n",
    "print(\"Waiting for job completion...\")\n",
    "print(\"(You can safely interrupt this cell and monitor in the portal)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Stream job logs (this will block until completion)\n",
    "ml_client.jobs.stream(returned_job.name)\n",
    "\n",
    "# Get final job status\n",
    "job_status = ml_client.jobs.get(returned_job.name)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Job Status: {job_status.status}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd38e5",
   "metadata": {},
   "source": [
    "## 13. Get Job Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ee07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final job status\n",
    "final_job = ml_client.jobs.get(returned_job.name)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Job Results\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Job Name: {final_job.name}\")\n",
    "print(f\"Status: {final_job.status}\")\n",
    "print(f\"Studio URL: {final_job.studio_url}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if final_job.status == \"Completed\":\n",
    "    print(\"\\n✓ Training completed successfully!\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. View detailed results in Azure ML Studio\")\n",
    "    print(\"2. Download the best model\")\n",
    "    print(\"3. Generate model explanations (see next cell)\")\n",
    "    print(\"4. Deploy the model for inference\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Job finished with status: {final_job.status}\")\n",
    "    print(\"Check the Studio URL for more details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ee0ef5",
   "metadata": {},
   "source": [
    "## 14. Generate Model Explanations (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not INTERPRETABILITY_AVAILABLE:\n",
    "    print(\"⚠ Interpretability libraries not available.\")\n",
    "    print(\"Install with: pip install interpret-community shap\")\n",
    "else:\n",
    "    print(\"Model explanation template:\")\n",
    "    print(\"\")\n",
    "    print(\"# After downloading the best model, you can generate explanations:\")\n",
    "    print(\"\")\n",
    "    print(\"# 1. Download and load the best model\")\n",
    "    print(\"# best_model = ml_client.models.download(name='your_model_name', version='latest', download_path='./models')\")\n",
    "    print(\"\")\n",
    "    print(\"# 2. Load and preprocess the data (same as training)\")\n",
    "    print(\"# df = load_and_preprocess_data(config)\")\n",
    "    print(\"# feature_columns = [col for col in df.columns if col != data_config['label_column']]\")\n",
    "    print(\"# X = df[feature_columns]\")\n",
    "    print(\"# y = df[data_config['label_column']]\")\n",
    "    print(\"\")\n",
    "    print(\"# 3. Create SHAP explainer\")\n",
    "    print(\"# explainer = shap.Explainer(best_model, X.sample(min(1000, len(X))))\")\n",
    "    print(\"# shap_values = explainer(X.sample(min(100, len(X))))\")\n",
    "    print(\"\")\n",
    "    print(\"# 4. Generate SHAP plots\")\n",
    "    print(\"# shap.summary_plot(shap_values, X.sample(min(100, len(X))), show=True)\")\n",
    "    print(\"\")\n",
    "    print(\"For complete explanation code, see generate_explanations.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee554e2",
   "metadata": {},
   "source": [
    "## 15. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f70f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files if needed\n",
    "if preprocessed_path and not config.get('output', {}).get('save_preprocessed_data', True):\n",
    "    if Path(preprocessed_path).exists():\n",
    "        Path(preprocessed_path).unlink()\n",
    "        print(f\"✓ Temporary file removed: {preprocessed_path}\")\n",
    "else:\n",
    "    print(\"No temporary files to clean up\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
